{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HPC 6.2 Automating the workflow on ARC.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1_-pO-S5d3b"
      },
      "source": [
        "# HPC 6.2 Automating the workflow on ARC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAb-Kfgm6jkA"
      },
      "source": [
        "We are now going to transfer this workflow over to ARC, automating parts of it as we go.\n",
        "\n",
        "The first step is to connect to ARC via `ssh`, as we are off campus we need to do this via `remote-access.leeds.ac.uk` first:\n",
        "\n",
        "`ssh <username>@remote-access.leeds.ac.uk`    \n",
        "`ssh <username>@arc4.leeds.ac.uk`\n",
        "\n",
        "We recommend to follow the instructions in our KB article to make this process a bit easier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgXQAPyg7uRR"
      },
      "source": [
        "We are going to use Anaconda to download and install our tools.  First:\n",
        "\n",
        "`module load anaconda`\n",
        "\n",
        "`conda config --add channels bioconda`\n",
        "\n",
        "Next, we are going to create an **Anaconda Environment** just for our experiment but in order to do this we need to put a list of the tools we need inside a file called `requirements.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jwlGCSR8HYC"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "fastqc\n",
        "trimmomatic\n",
        "bwa\n",
        "samtools=1.12\n",
        "bcftools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZisqYbC84og"
      },
      "source": [
        "and once we have that we need to tell Anaconda to make the environment based on what's in the file.\n",
        "\n",
        "`conda env create --name varcall --file requirements.txt`\n",
        "\n",
        "Which might take a few minutes.\n",
        "\n",
        "Once it has completed we can check if that environment has been created:\n",
        "\n",
        "`conda env list`\n",
        "\n",
        "and change over in to it:\n",
        "\n",
        "`source activate varcall`\n",
        "\n",
        "Just like before, we can check if the tools work at the command line:\n",
        "\n",
        "`bwa`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8cNmgtoA4AU"
      },
      "source": [
        "The final stage of the initial setup is to create a directory on `/nobackup` for our project work:\n",
        "\n",
        "`mkdir -p /nobackup/$USER/vc_pipeline/data/untrimmed_fastq/`\n",
        "\n",
        "This will give us a project directory (`vc_pipeline`) our data directory and a directory `untrimmed_fastq/` within that for our source data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo70LogEBU63"
      },
      "source": [
        "## Part 1: Downloading data\n",
        "\n",
        "Our first HPC job is to download the data and put it inside this directory we just created.  We could do this manually but it's more useful to do this with a `batch job`.\n",
        "\n",
        "We have a special `data` queue to do this so we don't clog up the regular queues with non-computational jobs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE62GDcTBvCc"
      },
      "source": [
        "%%writefile 1_download_data.sh\n",
        "# Submission script for download job\n",
        "# Martin Callaghan yyyy-mm-dd\n",
        "\n",
        "# Run from the current directory and with current environment\n",
        "#$ -cwd -V\n",
        "\n",
        "# Ask for some time (hh:mm:ss max of 48:00:00)\n",
        "#$ -l h_rt=01:00:00\n",
        "\n",
        "# Run in the data queue\n",
        "#$ -P data\n",
        "\n",
        "# Ask for some memory (by default, 1G, without a request)\n",
        "#$ -l h_vmem=2G\n",
        "\n",
        "# Send emails when job starts and ends\n",
        "#$ -m be\n",
        "\n",
        "# Now run the job\n",
        "cd /nobackup/$USER/vc_pipeline/data/untrimmed_fastq/\n",
        "\n",
        "curl -O ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR258/004/SRR2589044/SRR2589044_1.fastq.gz\n",
        "curl -O ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR258/004/SRR2589044/SRR2589044_2.fastq.gz\n",
        "curl -O ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR258/003/SRR2584863/SRR2584863_1.fastq.gz\n",
        "curl -O ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR258/003/SRR2584863/SRR2584863_2.fastq.gz\n",
        "curl -O ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR258/006/SRR2584866/SRR2584866_1.fastq.gz\n",
        "curl -O ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR258/006/SRR2584866/SRR2584866_2.fastq.gz "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MKpXu7cHGnl"
      },
      "source": [
        "We can now send this job to the queue to be executed by the **batch scheduler**:\n",
        "\n",
        "`qsub download_data.sh`\n",
        "\n",
        "and you can check the progress of the job with the command:\n",
        "\n",
        "`qstat`\n",
        "\n",
        "When it's completed, you'll be able to go and see if the files have been downloaded:\n",
        "\n",
        "`ls -l /nobackup/$USER/vc_pipeline/data/untrimmed_fastq/`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hguAm6K3WyWv"
      },
      "source": [
        "At the momnt, the script we wrote is just in our home directory.  We should put it inside our project directory in a new `scripts` subdirectory.\n",
        "\n",
        "`mkdir /nobackup/$USER/vc_pipeline/scripts/`  \n",
        "`mv download_data.sh /nobackup/$USER/vc_pipeline/scripts/`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrxZS1xfWoMo"
      },
      "source": [
        "## Part 2: Analysing quality with fastqc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j0Gn0xNX6SW"
      },
      "source": [
        "Let's create a script using nano to run the `fastqc` part of the pipeline:\n",
        "\n",
        "`nano /nobackup/$USER/vc_pipeline/scripts/read_qc.sh`\n",
        "\n",
        "`fastqc` can use multiple cores so we can amend our submission script accordingly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3Qz6_WZ5X_F"
      },
      "source": [
        "%%writefile 2_read_qc.sh\n",
        "# Submission script for fastqc job\n",
        "# Martin Callaghan yyyy-mm-dd\n",
        "\n",
        "# Run from the current directory and with current environment\n",
        "#$ -cwd -V\n",
        "\n",
        "# Ask for some time (hh:mm:ss max of 48:00:00)\n",
        "#$ -l h_rt=01:00:00\n",
        "\n",
        "# Ask for some memory (by default, 1G, without a request)\n",
        "#$ -l h_vmem=4G\n",
        "\n",
        "# Request 4 cores\n",
        "#$ -pe smp 4\n",
        "\n",
        "# Send emails when job starts and ends\n",
        "#$ -m be\n",
        "\n",
        "# Now run the job\n",
        "module load anaconda\n",
        "source activate varcall\n",
        "\n",
        "cd /nobackup/$USER/vc_pipeline/data/untrimmed_fastq/\n",
        "\n",
        "echo \"Running FastQC ...\"\n",
        "fastqc *.fastq*\n",
        "\n",
        "mkdir -p /nobackup/$USER/vc_pipeline/results/fastqc_untrimmed_reads\n",
        "\n",
        "echo \"Saving FastQC results...\"\n",
        "mv *.zip /nobackup/$USER/vc_pipeline/results/fastqc_untrimmed_reads/\n",
        "mv *.html /nobackup/$USER/vc_pipeline/results/fastqc_untrimmed_reads/\n",
        "\n",
        "cd /nobackup/$USER/vc_pipeline/results/fastqc_untrimmed_reads/\n",
        "\n",
        "echo \"Unzipping...\"\n",
        "for filename in *.zip\n",
        "    do\n",
        "    unzip $filename\n",
        "    done\n",
        "\n",
        "echo \"Saving summary...\"\n",
        "mkdir /nobackup/$USER/vc_pipeline/docs\n",
        "cat */summary.txt > /nobackup/$USER/vc_pipeline/docs/fastqc_summaries.txt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebABBE6NZo3g"
      },
      "source": [
        "Now we can submit the job and wait for it to be completed:\n",
        "\n",
        "`qsub /nobackup/$USER/vc_pipeline/scripts/read_qc.sh`\n",
        "\n",
        "Remembering that `qstat` will show us the progress of the job.\n",
        "\n",
        "**NOTE**: At this stage in real life you would need to do some assessment of the quality of your reads to determine the parameters of the `trimmomatic` command."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CXZsUAYc5az"
      },
      "source": [
        "## Part 3: Trimming\n",
        "\n",
        "Just like in our previous Notebook, we now need to call `trimmomatic` on all of our **untrimmed** data to create the **trimmed** data for the next step.\n",
        "\n",
        "Create this file using nano:\n",
        "\n",
        "`nano /nobackup/issmcal/vc_pipeline/scripts/run_trims.sh`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klq8TGIodurs"
      },
      "source": [
        "%%writefile 3_run_trims.sh\n",
        "# Submission script for trimming job\n",
        "# Martin Callaghan yyyy-mm-dd\n",
        "\n",
        "# Run from the current directory and with current environment\n",
        "#$ -cwd -V\n",
        "\n",
        "# Ask for some time (hh:mm:ss max of 48:00:00)\n",
        "#$ -l h_rt=02:00:00\n",
        "\n",
        "# Ask for some memory (by default, 1G, without a request)\n",
        "#$ -l h_vmem=4G\n",
        "\n",
        "# Request 4 cores\n",
        "#$ -pe smp 4\n",
        "\n",
        "# Send emails when job starts and ends\n",
        "#$ -m be\n",
        "\n",
        "# Now run the job\n",
        "# Change to the correct directory\n",
        "cd /nobackup/$USER/vc_pipeline/data/untrimmed_fastq\n",
        "\n",
        "# Download the Nextera adapter\n",
        "! wget https://raw.githubusercontent.com/timflutre/trimmomatic/master/adapters/NexteraPE-PE.fa\n",
        "\n",
        "# Loop over the untrimmed fastq files\n",
        "\n",
        "for infile in *_1.fastq.gz\n",
        "do\n",
        "   base=$(basename ${infile} _1.fastq.gz)\n",
        "   trimmomatic PE ${infile} ${base}_2.fastq.gz \\\n",
        "                ${base}_1.trim.fastq.gz ${base}_1un.trim.fastq.gz \\\n",
        "                ${base}_2.trim.fastq.gz ${base}_2un.trim.fastq.gz \\\n",
        "                SLIDINGWINDOW:4:20 MINLEN:25 ILLUMINACLIP:NexteraPE-PE.fa:2:40:15 \n",
        "done\n",
        "\n",
        "# move our trimmed FASTQ files to a new subdirectory within our data/ directory\n",
        "mkdir -p /nobackup/$USER/vc_pipeline/data/trimmed_fastq\n",
        "mv *.trim* /nobackup/$USER/vc_pipeline/data/trimmed_fastq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj7gwBUaaE4H"
      },
      "source": [
        "## Part 4: Automating the rest of the Variant Calling workflow\n",
        "\n",
        "We can extend these principles to the entire variant calling workflow. To do this, we will take all of the individual commands that we wrote before, put them into a single file, add variables so that the script knows to iterate through our input files and write to the appropriate output files. \n",
        "\n",
        "This is very similar to what we did with our `read_qc.sh` script, but will be a bit more complex.\n",
        "\n",
        "Again, using `nano`:\n",
        "\n",
        "`nano /nobackup/issmcal/vc_pipeline/scripts/vc_script.sh`\n",
        "\n",
        "Submit to the queue with:\n",
        "\n",
        "`qsub /nobackup/issmcal/vc_pipeline/scripts/vc_script.sh`\n",
        "\n",
        "And watch progress with `qstat`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOmHip_xEKZa",
        "outputId": "f87faf1b-721f-433b-8fe7-e1f6fc062221"
      },
      "source": [
        "%%writefile 4_vc_script.sh\n",
        "# Submission script for vc workflow job\n",
        "# Martin Callaghan yyyy-mm-dd\n",
        "\n",
        "# Run from the current directory and with current environment\n",
        "#$ -cwd -V\n",
        "\n",
        "# Ask for some time (hh:mm:ss max of 48:00:00)\n",
        "#$ -l h_rt=02:00:00\n",
        "\n",
        "# Ask for some memory (by default, 1G, without a request)\n",
        "#$ -l h_vmem=4G\n",
        "\n",
        "# Request 4 cores\n",
        "#$ -pe smp 4\n",
        "\n",
        "# Send emails when job starts and ends\n",
        "#$ -m be\n",
        "\n",
        "# Now run the job\n",
        "module load anaconda\n",
        "source activate varcall\n",
        "\n",
        "# Download the reference genome\n",
        "mkdir -p /nobackup/$USER/vc_pipeline/data/ref_genome\n",
        "curl -L -o /nobackup/$USER/vc_pipeline/data/ref_genome/ecoli_rel606.fasta.gz ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/017/985/GCA_000017985.1_ASM1798v1/GCA_000017985.1_ASM1798v1_genomic.fna.gz\n",
        "gunzip /nobackup/$USER/vc_pipeline/data/ref_genome/ecoli_rel606.fasta.gz\n",
        "\n",
        "cd /nobackup/$USER/vc_pipeline/results\n",
        "\n",
        "genome=/nobackup/$USER/vc_pipeline/data/ref_genome/ecoli_rel606.fasta\n",
        "\n",
        "bwa index $genome\n",
        "\n",
        "mkdir -p sam bam bcf vcf\n",
        "\n",
        "for fq1 in /nobackup/$USER/vc_pipeline/data/trimmed_fastq/*_1.trim.fastq*\n",
        "    do\n",
        "    echo \"working with file $fq1\"\n",
        "\n",
        "    base=$(basename $fq1 _1.trim.fastq)\n",
        "    echo \"base name is $base\"\n",
        "\n",
        "    fq1=/nobackup/$USER/vc_pipeline/data/trimmed_fastq/${base}_1.trim.fastq\n",
        "    fq2=/nobackup/$USER/vc_pipeline/data/trimmed_fastq/${base}_2.trim.fastq\n",
        "\n",
        "    sam=/nobackup/$USER/vc_pipeline/results/sam/${base}.aligned.sam\n",
        "    bam=/nobackup/$USER/vc_pipeline/results/bam/${base}.aligned.bam\n",
        "\n",
        "    sorted_bam=/nobackup/$USER/vc_pipeline/results/bam/${base}.aligned.sorted.bam\n",
        "    raw_bcf=/nobackup/$USER/vc_pipeline/results/bcf/${base}_raw.bcf\n",
        "\n",
        "    variants=/nobackup/$USER/vc_pipeline/results/bcf/${base}_variants.vcf\n",
        "    final_variants=/nobackup/$USER/vc_pipeline/results/vcf/${base}_final_variants.vcf \n",
        "\n",
        "    bwa mem $genome $fq1 $fq2 > $sam\n",
        "\n",
        "    samtools view -S -b $sam > $bam\n",
        "    samtools sort -o $sorted_bam $bam \n",
        "\n",
        "    samtools index $sorted_bam\n",
        "\n",
        "    bcftools mpileup -O b -o $raw_bcf -f $genome $sorted_bam\n",
        "    bcftools call --ploidy 1 -m -v -o $variants $raw_bcf \n",
        "    vcfutils.pl varFilter $variants > $final_variants\n",
        "   \n",
        "    done"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing 4_vc_script.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMVHRWXivTE9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}